{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, target_names, class_indices):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Initialize lists to store precision, recall, F1-score, and support for each class\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "\n",
    "    # Calculate precision, recall, F1-score for each class\n",
    "    for i in range(len(target_names)):\n",
    "        true_positives = cm[i, i]\n",
    "        false_positives = cm[:, i].sum() - true_positives\n",
    "        false_negatives = cm[i, :].sum() - true_positives\n",
    "        true_negatives = cm.sum() - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        if true_positives + false_positives > 0:\n",
    "            precision_i = true_positives / (true_positives + false_positives)\n",
    "        else:\n",
    "            precision_i = 0.0\n",
    "\n",
    "        # Recall: TP / (TP + FN)\n",
    "        if true_positives + false_negatives > 0:\n",
    "            recall_i = true_positives / (true_positives + false_negatives)\n",
    "        else:\n",
    "            recall_i = 0.0\n",
    "\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        if precision_i + recall_i > 0:\n",
    "            f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i)\n",
    "        else:\n",
    "            f1_i = 0.0\n",
    "\n",
    "        # Support: The number of true instances of each class\n",
    "        support_i = cm[i, :].sum()\n",
    "\n",
    "        # Append calculated metrics for this class\n",
    "        precision.append(precision_i)\n",
    "        recall.append(recall_i)\n",
    "        f1_score.append(f1_i)\n",
    "        support.append(support_i)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "    # Calculate average F1-score for specified classes\n",
    "    f1_average = np.mean([f1_score[i] for i in class_indices])\n",
    "\n",
    "    # Print the aesthetically improved report\n",
    "    print(\"\\n\" + \"Classification Report\".center(65, \"=\"))\n",
    "    print(f\"{'Class':<15}{'Precision':>12}{'Recall':>12}{'F1-Score':>12}{'Support':>12}\")\n",
    "    print(\"=\" * 65)\n",
    "    for i, label in enumerate(target_names):\n",
    "        print(f\"{label:<15}{precision[i]:>12.4f}{recall[i]:>12.4f}{f1_score[i]:>12.4f}{support[i]:>12}\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"Average F1-Score for classes : {f1_average:.4f}\")\n",
    "    print(f\"Accuracy : {accuracy:.4f}\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "class_indices = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,  # Number of classes\n",
    "    'learning_rate': 0.001,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.6478457734751482,\n",
    "    'colsample_bytree': 0.9431214021788126,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "def xgBoostPrediction(X_train, y_train, X_test, y_test, params, num_boost_round=462, target_names=None):\n",
    "\n",
    "    train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "    test_data = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    xgb_model = xgb.train(params, train_data,num_boost_round=num_boost_round)\n",
    "\n",
    "    # Predict the class labels\n",
    "    y_pred = xgb_model.predict(test_data)\n",
    "    \n",
    "    print('Expected features:', xgb_model.feature_names)\n",
    "    print('Dataset columns:', X_train.columns.tolist())\n",
    "\n",
    "    # Generate and print classification report\n",
    "    print(\"xgBoost results:\")\n",
    "    custom_classification_report(y_test, y_pred, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rfPrediction(X_train, y_train, X_test, y_test, n_estimators=100, max_depth=None, target_names=None):\n",
    "    # creating a RF classifier\n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth, random_state=42)  \n",
    "    \n",
    "    # Training the model on the training dataset\n",
    "    # fit function is used to train the model using the training sets as parameters\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # performing predictions on the test dataset\n",
    "    y_pred_rf = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Random Forest results:\")\n",
    "    custom_classification_report(y_test, y_pred_rf, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logisticPrediction(X_train, y_train, X_test, y_test, max_iter=1000, target_names=None):\n",
    "    # creating a RF classifier\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=max_iter)\n",
    "    \n",
    "    # Training the model on the training dataset\n",
    "    # fit function is used to train the model using the training sets as parameters\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # performing predictions on the test dataset\n",
    "    y_pred_log_reg = log_reg.predict(X_test)\n",
    "    \n",
    "    print(\"Logistic Regression results:\")\n",
    "    custom_classification_report(y_test, y_pred_log_reg, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure h0 and c0 are on the same device as input\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(self.dropout(out[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using ParameterGrid\n",
    "    \n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training input data\n",
    "        y_train (torch.Tensor): Training labels\n",
    "        X_val (torch.Tensor): Validation input data\n",
    "        y_val (torch.Tensor): Validation labels\n",
    "        column (list, optional): Feature column names\n",
    "    \n",
    "    Returns:\n",
    "        dict: Best hyperparameters\n",
    "    \"\"\"\n",
    "    # Convert to numpy if tensor\n",
    "    X_train = X_train.numpy() if torch.is_tensor(X_train) else X_train\n",
    "    y_train = y_train.numpy() if torch.is_tensor(y_train) else y_train\n",
    "    X_val = X_val.numpy() if torch.is_tensor(X_val) else X_val\n",
    "    y_val = y_val.numpy() if torch.is_tensor(y_val) else y_val\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    param_grid = {\n",
    "        'hidden_size': [32, 64],\n",
    "        'num_layers': [2, 4],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [1 ,2, 4],\n",
    "        'dropout': [0.0, 0.2],\n",
    "        'weight_decay': [0],\n",
    "        'num_epochs': [20, 30, 50]\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations\n",
    "    param_combinations = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Prepare input parameters\n",
    "    input_size = X_train_tensor.shape[2]\n",
    "    num_classes = len(torch.unique(y_train_tensor))\n",
    "    \n",
    "    # Search through hyperparameters\n",
    "    for params in param_combinations:\n",
    "        # Create model\n",
    "        model = LSTMModel(\n",
    "            input_size=input_size,\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            num_classes=num_classes,\n",
    "            dropout=params['dropout']\n",
    "        )\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=params['learning_rate'], \n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_accuracy = 0\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += y_batch.size(0)\n",
    "                    correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            val_accuracy = 100 * correct / total\n",
    "            best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "        \n",
    "        # Store results\n",
    "        results.append((params, best_val_accuracy))\n",
    "        best_params, best_accuracy = max(results, key=lambda x: x[1])\n",
    "        print(f\"Current Best Hyperparameters: {best_params}\")\n",
    "        print(f\"Current Best Validation Accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    # Find best hyperparameters\n",
    "    best_params, best_accuracy = max(results, key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed_data'):\n",
    "    os.makedirs('processed_data')\n",
    "\n",
    "list_of_data = []\n",
    "\n",
    "    \n",
    "for data in os.listdir('processed_data'):\n",
    "    list_of_data.append(f\"processed_data\\\\{data}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataEvaluation(dataset):\n",
    "    transform_eeg_data = pd.read_csv(dataset)\n",
    "    le = LabelEncoder()\n",
    "    transform_eeg_data['GROUP'] = le.fit_transform(transform_eeg_data['GROUP'])\n",
    "    # Convert object columns to numeric\n",
    "    for col in transform_eeg_data.columns:\n",
    "        if transform_eeg_data[col].dtype == 'object':\n",
    "            transform_eeg_data[col] = pd.to_numeric(transform_eeg_data[col], errors='coerce')\n",
    "\n",
    "    # Features and target\n",
    "    X = transform_eeg_data.drop('GROUP', axis=1)\n",
    "    y = transform_eeg_data['GROUP']\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "    target_names=list(map(str,le.classes_))\n",
    "\n",
    "    file_name = dataset.split('.')[0]\n",
    "    print(f\"For {file_name} data\")\n",
    "\n",
    "    # xgBoostPrediction(X_train, y_train, X_test, y_test, target_names=target_names, params=params)\n",
    "    # rfPrediction(X_train, y_train, X_test,  y_test, target_names=target_names, max_depth=100)\n",
    "    # logisticPrediction(X_train, y_train, X_test, y_test, target_names=target_names)\n",
    "\n",
    "\n",
    "\n",
    "    # Prepare the data\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # {'batch_size': 1, 'dropout': 0.2, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
    "    # Best Hyperparameters: {'batch_size': 4, 'dropout': 0.2, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 30, 'num_layers': 2, 'weight_decay': 0}\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 64\n",
    "    num_layers = 4\n",
    "    num_classes = 3\n",
    "    dropout = 0.2\n",
    "\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "\n",
    "\n",
    "    print(\"LSTM results:\")\n",
    "    custom_classification_report(y_true, y_pred, target_names=target_names,class_indices = [0, 1, 2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For processed_data\\Non_Standardized_data data\n",
      "Epoch [1/50], Loss: 0.9328\n",
      "Epoch [2/50], Loss: 0.8408\n",
      "Epoch [3/50], Loss: 0.7188\n",
      "Epoch [4/50], Loss: 0.5676\n",
      "Epoch [5/50], Loss: 0.4745\n",
      "Epoch [6/50], Loss: 0.5512\n",
      "Epoch [7/50], Loss: 0.5691\n",
      "Epoch [8/50], Loss: 0.5483\n",
      "Epoch [9/50], Loss: 0.5314\n",
      "Epoch [10/50], Loss: 0.5659\n",
      "Epoch [11/50], Loss: 0.6115\n",
      "Epoch [12/50], Loss: 0.6332\n",
      "Epoch [13/50], Loss: 0.5998\n",
      "Epoch [14/50], Loss: 0.5352\n",
      "Epoch [15/50], Loss: 0.6872\n",
      "Epoch [16/50], Loss: 0.6058\n",
      "Epoch [17/50], Loss: 0.6161\n",
      "Epoch [18/50], Loss: 0.6208\n",
      "Epoch [19/50], Loss: 0.6251\n",
      "Epoch [20/50], Loss: 0.6154\n",
      "Epoch [21/50], Loss: 0.5884\n",
      "Epoch [22/50], Loss: 0.6470\n",
      "Epoch [23/50], Loss: 0.6541\n",
      "Epoch [24/50], Loss: 0.6787\n",
      "Epoch [25/50], Loss: 0.6973\n",
      "Epoch [26/50], Loss: 0.6158\n",
      "Epoch [27/50], Loss: 0.7472\n",
      "Epoch [28/50], Loss: 0.6923\n",
      "Epoch [29/50], Loss: 0.6994\n",
      "Epoch [30/50], Loss: 0.6962\n",
      "Epoch [31/50], Loss: 0.7075\n",
      "Epoch [32/50], Loss: 0.7473\n",
      "Epoch [33/50], Loss: 0.7129\n",
      "Epoch [34/50], Loss: 0.6501\n",
      "Epoch [35/50], Loss: 0.6176\n",
      "Epoch [36/50], Loss: 0.6647\n",
      "Epoch [37/50], Loss: 0.6678\n",
      "Epoch [38/50], Loss: 0.6628\n",
      "Epoch [39/50], Loss: 0.6877\n",
      "Epoch [40/50], Loss: 0.6624\n",
      "Epoch [41/50], Loss: 0.6695\n",
      "Epoch [42/50], Loss: 0.5793\n",
      "Epoch [43/50], Loss: 0.6326\n",
      "Epoch [44/50], Loss: 0.6096\n",
      "Epoch [45/50], Loss: 0.6695\n",
      "Epoch [46/50], Loss: 0.6390\n",
      "Epoch [47/50], Loss: 0.5616\n",
      "Epoch [48/50], Loss: 0.6251\n",
      "Epoch [49/50], Loss: 0.6254\n",
      "Epoch [50/50], Loss: 0.5412\n",
      "LSTM results:\n",
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.4118      0.8750      0.5600           8\n",
      "1                    0.5000      0.1111      0.1818           9\n",
      "2                    1.0000      0.3333      0.5000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.4139\n",
      "Accuracy : 0.4500\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "DataEvaluation(list_of_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For processed_data\\Pure_data data\n",
      "Epoch [1/50], Loss: 1.0397\n",
      "Epoch [2/50], Loss: 0.9507\n",
      "Epoch [3/50], Loss: 0.6380\n",
      "Epoch [4/50], Loss: 0.4953\n",
      "Epoch [5/50], Loss: 0.5749\n",
      "Epoch [6/50], Loss: 0.6319\n",
      "Epoch [7/50], Loss: 0.5310\n",
      "Epoch [8/50], Loss: 0.5979\n",
      "Epoch [9/50], Loss: 0.5504\n",
      "Epoch [10/50], Loss: 0.5648\n",
      "Epoch [11/50], Loss: 0.5841\n",
      "Epoch [12/50], Loss: 0.5992\n",
      "Epoch [13/50], Loss: 0.5888\n",
      "Epoch [14/50], Loss: 0.6136\n",
      "Epoch [15/50], Loss: 0.5703\n",
      "Epoch [16/50], Loss: 0.6200\n",
      "Epoch [17/50], Loss: 0.6047\n",
      "Epoch [18/50], Loss: 0.5395\n",
      "Epoch [19/50], Loss: 0.5932\n",
      "Epoch [20/50], Loss: 0.6539\n",
      "Epoch [21/50], Loss: 0.6120\n",
      "Epoch [22/50], Loss: 0.5811\n",
      "Epoch [23/50], Loss: 0.5604\n",
      "Epoch [24/50], Loss: 0.5964\n",
      "Epoch [25/50], Loss: 0.6254\n",
      "Epoch [26/50], Loss: 0.6039\n",
      "Epoch [27/50], Loss: 0.5818\n",
      "Epoch [28/50], Loss: 0.5998\n",
      "Epoch [29/50], Loss: 0.5076\n",
      "Epoch [30/50], Loss: 0.4987\n",
      "Epoch [31/50], Loss: 0.5729\n",
      "Epoch [32/50], Loss: 0.5970\n",
      "Epoch [33/50], Loss: 0.6168\n",
      "Epoch [34/50], Loss: 0.6270\n",
      "Epoch [35/50], Loss: 0.5432\n",
      "Epoch [36/50], Loss: 0.6177\n",
      "Epoch [37/50], Loss: 0.5336\n",
      "Epoch [38/50], Loss: 0.5425\n",
      "Epoch [39/50], Loss: 0.6377\n",
      "Epoch [40/50], Loss: 0.5611\n",
      "Epoch [41/50], Loss: 0.5647\n",
      "Epoch [42/50], Loss: 0.5962\n",
      "Epoch [43/50], Loss: 0.5278\n",
      "Epoch [44/50], Loss: 0.5910\n",
      "Epoch [45/50], Loss: 0.5490\n",
      "Epoch [46/50], Loss: 0.5304\n",
      "Epoch [47/50], Loss: 0.5487\n",
      "Epoch [48/50], Loss: 0.6216\n",
      "Epoch [49/50], Loss: 0.6062\n",
      "Epoch [50/50], Loss: 0.4502\n",
      "LSTM results:\n",
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.4000      1.0000      0.5714           8\n",
      "1                    0.0000      0.0000      0.0000           9\n",
      "2                    0.0000      0.0000      0.0000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.1905\n",
      "Accuracy : 0.4000\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "DataEvaluation(list_of_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For processed_data\\Standardized_data data\n",
      "Epoch [1/50], Loss: 1.0887\n",
      "Epoch [2/50], Loss: 1.0223\n",
      "Epoch [3/50], Loss: 0.7985\n",
      "Epoch [4/50], Loss: 0.4032\n",
      "Epoch [5/50], Loss: 0.2401\n",
      "Epoch [6/50], Loss: 0.2474\n",
      "Epoch [7/50], Loss: 0.2609\n",
      "Epoch [8/50], Loss: 0.1858\n",
      "Epoch [9/50], Loss: 0.2220\n",
      "Epoch [10/50], Loss: 0.2111\n",
      "Epoch [11/50], Loss: 0.0821\n",
      "Epoch [12/50], Loss: 0.1488\n",
      "Epoch [13/50], Loss: 0.1556\n",
      "Epoch [14/50], Loss: 0.1375\n",
      "Epoch [15/50], Loss: 0.0592\n",
      "Epoch [16/50], Loss: 0.0350\n",
      "Epoch [17/50], Loss: 0.0322\n",
      "Epoch [18/50], Loss: 0.0300\n",
      "Epoch [19/50], Loss: 0.0107\n",
      "Epoch [20/50], Loss: 0.0200\n",
      "Epoch [21/50], Loss: 0.0081\n",
      "Epoch [22/50], Loss: 0.0128\n",
      "Epoch [23/50], Loss: 0.0184\n",
      "Epoch [24/50], Loss: 0.0039\n",
      "Epoch [25/50], Loss: 0.0130\n",
      "Epoch [26/50], Loss: 0.0042\n",
      "Epoch [27/50], Loss: 0.0031\n",
      "Epoch [28/50], Loss: 0.0102\n",
      "Epoch [29/50], Loss: 0.0028\n",
      "Epoch [30/50], Loss: 0.0064\n",
      "Epoch [31/50], Loss: 0.0092\n",
      "Epoch [32/50], Loss: 0.0012\n",
      "Epoch [33/50], Loss: 0.0079\n",
      "Epoch [34/50], Loss: 0.0107\n",
      "Epoch [35/50], Loss: 0.0020\n",
      "Epoch [36/50], Loss: 0.0006\n",
      "Epoch [37/50], Loss: 0.0006\n",
      "Epoch [38/50], Loss: 0.0010\n",
      "Epoch [39/50], Loss: 0.0010\n",
      "Epoch [40/50], Loss: 0.0011\n",
      "Epoch [41/50], Loss: 0.0007\n",
      "Epoch [42/50], Loss: 0.0008\n",
      "Epoch [43/50], Loss: 0.0015\n",
      "Epoch [44/50], Loss: 0.0004\n",
      "Epoch [45/50], Loss: 0.0001\n",
      "Epoch [46/50], Loss: 0.0007\n",
      "Epoch [47/50], Loss: 0.0006\n",
      "Epoch [48/50], Loss: 0.0006\n",
      "Epoch [49/50], Loss: 0.0005\n",
      "Epoch [50/50], Loss: 0.0003\n",
      "LSTM results:\n",
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.7000      0.8750      0.7778           8\n",
      "1                    0.6667      0.2222      0.3333           9\n",
      "2                    0.4286      1.0000      0.6000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.5704\n",
      "Accuracy : 0.6000\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "DataEvaluation(list_of_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For processed_data\\Standardized_pure_data data\n",
      "Epoch [1/50], Loss: 1.0590\n",
      "Epoch [2/50], Loss: 0.9391\n",
      "Epoch [3/50], Loss: 0.6203\n",
      "Epoch [4/50], Loss: 0.2913\n",
      "Epoch [5/50], Loss: 0.1240\n",
      "Epoch [6/50], Loss: 0.0904\n",
      "Epoch [7/50], Loss: 0.0377\n",
      "Epoch [8/50], Loss: 0.0163\n",
      "Epoch [9/50], Loss: 0.0061\n",
      "Epoch [10/50], Loss: 0.0047\n",
      "Epoch [11/50], Loss: 0.0023\n",
      "Epoch [12/50], Loss: 0.0021\n",
      "Epoch [13/50], Loss: 0.0107\n",
      "Epoch [14/50], Loss: 0.0017\n",
      "Epoch [15/50], Loss: 0.0038\n",
      "Epoch [16/50], Loss: 0.0006\n",
      "Epoch [17/50], Loss: 0.0002\n",
      "Epoch [18/50], Loss: 0.0010\n",
      "Epoch [19/50], Loss: 0.0010\n",
      "Epoch [20/50], Loss: 0.0002\n",
      "Epoch [21/50], Loss: 0.0014\n",
      "Epoch [22/50], Loss: 0.0012\n",
      "Epoch [23/50], Loss: 0.0046\n",
      "Epoch [24/50], Loss: 0.0005\n",
      "Epoch [25/50], Loss: 0.0005\n",
      "Epoch [26/50], Loss: 0.0001\n",
      "Epoch [27/50], Loss: 0.0003\n",
      "Epoch [28/50], Loss: 0.0005\n",
      "Epoch [29/50], Loss: 0.0003\n",
      "Epoch [30/50], Loss: 0.0001\n",
      "Epoch [31/50], Loss: 0.0001\n",
      "Epoch [32/50], Loss: 0.0001\n",
      "Epoch [33/50], Loss: 0.0001\n",
      "Epoch [34/50], Loss: 0.0004\n",
      "Epoch [35/50], Loss: 0.0002\n",
      "Epoch [36/50], Loss: 0.0005\n",
      "Epoch [37/50], Loss: 0.0002\n",
      "Epoch [38/50], Loss: 0.0001\n",
      "Epoch [39/50], Loss: 0.0002\n",
      "Epoch [40/50], Loss: 0.0001\n",
      "Epoch [41/50], Loss: 0.0003\n",
      "Epoch [42/50], Loss: 0.0004\n",
      "Epoch [43/50], Loss: 0.0002\n",
      "Epoch [44/50], Loss: 0.0000\n",
      "Epoch [45/50], Loss: 0.0002\n",
      "Epoch [46/50], Loss: 0.0002\n",
      "Epoch [47/50], Loss: 0.0013\n",
      "Epoch [48/50], Loss: 0.0001\n",
      "Epoch [49/50], Loss: 0.0007\n",
      "Epoch [50/50], Loss: 0.0008\n",
      "LSTM results:\n",
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.3846      0.6250      0.4762           8\n",
      "1                    0.4286      0.3333      0.3750           9\n",
      "2                    0.0000      0.0000      0.0000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.2837\n",
      "Accuracy : 0.4000\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "DataEvaluation(list_of_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Hyperparameter Tuning\n",
    "def LSTM_Hyper_tuning(dataset):\n",
    "    transform_eeg_data = pd.read_csv(dataset)\n",
    "    le = LabelEncoder()\n",
    "    transform_eeg_data['GROUP'] = le.fit_transform(transform_eeg_data['GROUP'])\n",
    "    # Convert object columns to numeric\n",
    "    for col in transform_eeg_data.columns:\n",
    "        if transform_eeg_data[col].dtype == 'object':\n",
    "            transform_eeg_data[col] = pd.to_numeric(transform_eeg_data[col], errors='coerce')\n",
    "\n",
    "    # Features and target\n",
    "    X = transform_eeg_data.drop('GROUP', axis=1)\n",
    "    y = transform_eeg_data['GROUP']\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "    target_names=list(map(str,le.classes_))\n",
    "\n",
    "    file_name = dataset.split('.')[0]\n",
    "    print(f\"For {file_name} data\")\n",
    "\n",
    "    # Prepare the data\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    print(\"\\n Hyperparameter Tuning on LSTM:\")\n",
    "    hyperparameter_tuning(X_train=X_train_tensor, y_train=y_train_tensor, X_val=X_test_tensor, y_val=y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def xg_Hyper_tuning(dataset):\n",
    "\n",
    "    transform_eeg_data = pd.read_csv(dataset)\n",
    "    le = LabelEncoder()\n",
    "    transform_eeg_data['GROUP'] = le.fit_transform(transform_eeg_data['GROUP'])\n",
    "    # Convert object columns to numeric\n",
    "    for col in transform_eeg_data.columns:\n",
    "        if transform_eeg_data[col].dtype == 'object':\n",
    "            transform_eeg_data[col] = pd.to_numeric(transform_eeg_data[col], errors='coerce')\n",
    "\n",
    "    # Features and target\n",
    "    X = transform_eeg_data.drop('GROUP', axis=1)\n",
    "    y = transform_eeg_data['GROUP']\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "    target_names=list(map(str,le.classes_))\n",
    "\n",
    "    file_name = dataset.split('.')[0]\n",
    "    print(f\"For {file_name} data\")\n",
    "\n",
    "    param_grid = {\n",
    "        'objective': ['multi:softmax'],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.001 ,0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'random_state': [42],\n",
    "        'num_class': [3]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Perform grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "        # Best parameters and score\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_accuracy = best_model.score(X_test, y_test)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For processed_data\\Standardized_data data\n",
      "\n",
      " Hyperparameter Tuning on LSTM:\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 65.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 70.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 70.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 70.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 75.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 80.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mLSTM_Hyper_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mLSTM_Hyper_tuning\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     28\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Hyperparameter Tuning on LSTM:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mhyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m(X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     84\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 85\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LSTM_Hyper_tuning(list_of_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Hyper_tuning(list_of_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For processed_data\\Standardized_pure_data data\n",
      "\n",
      " Hyperparameter Tuning on LSTM:\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 45.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 45.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 45.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 30, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 50.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 30, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 50.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 30, 'num_layers': 4, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 50.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 55.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 55.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 55.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 55.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 55.00%\n",
      "Current Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 2, 'weight_decay': 0}\n",
      "Current Best Validation Accuracy: 55.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mLSTM_Hyper_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mLSTM_Hyper_tuning\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     28\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Hyperparameter Tuning on LSTM:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mhyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m(X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m---> 93\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     25\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 27\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LSTM_Hyper_tuning(list_of_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First LSTM Hyperparameter Tuning on Standardized Data:\n",
    "***\n",
    "Param:\n",
    "    param_grid = {\n",
    "        'hidden_size': [64, 128],\n",
    "        'num_layers': [1, 2],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [4],\n",
    "        'dropout': [0.0, 0.2, 0.5],\n",
    "        'weight_decay': [0, 1e-5],\n",
    "        'num_epochs': [50, 70]\n",
    "    }\n",
    "-    Train_Shuffle = False\n",
    "-    Test_Shuffle = False\n",
    "\n",
    "- Best Hyperparameters: {'batch_size': 4, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 2, 'weight_decay': 0}\n",
    "- Best Validation Accuracy: 75.00%\n",
    "- Best Average F1-Score for classes : 0.6868\n",
    "***\n",
    "\n",
    "\n",
    "#### Second LSTM Hyperparameter Tuning on Standardized Data:\n",
    "***\n",
    "Param:\n",
    "    param_grid = {\n",
    "        'hidden_size': [64, 128, 256],\n",
    "        'num_layers': [2, 4, 8, 16],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [4, 8, 16],\n",
    "        'dropout': [0.0, 0.2],\n",
    "        'weight_decay': [0, 1e-4],\n",
    "        'num_epochs': [50]\n",
    "    }\n",
    "-    Train_Shuffle = False\n",
    "-    Test_Shuffle = False\n",
    "\n",
    "- Best Hyperparameters: {'batch_size': 4, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
    "- Best Validation Accuracy: 80.00%\n",
    "- Best Average F1-Score for classes : 0.8004\n",
    "***\n",
    "\n",
    "#### Third LSTM Hyperparameter Tuning on Standardized Data:\n",
    "***\n",
    "Param:\n",
    "    param_grid = {\n",
    "        'hidden_size': [64, 128],\n",
    "        'num_layers': [2, 4],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [1 ,2, 4],\n",
    "        'dropout': [0.0, 0.2],\n",
    "        'weight_decay': [0, 1e-4],\n",
    "        'num_epochs': [20, 30, 50]\n",
    "    }\n",
    "-    Train_Shuffle = False\n",
    "-    Test_Shuffle = False\n",
    "\n",
    "- Best Hyperparameters: {'batch_size': 1, 'dropout': 0.0, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 20, 'num_layers': 4, 'weight_decay': 0}\n",
    "- Best Validation Accuracy: 80.00%\n",
    "- Best Average F1-Score for classes : 0.8426\n",
    "***\n",
    "\n",
    "#### Fourth LSTM Hyperparameter Tuning on Standardized Data:\n",
    "***\n",
    "Param:\n",
    "    param_grid = {\n",
    "        'hidden_size': [32, 64, 128],\n",
    "        'num_layers': [2, 4],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [1 ,2, 4],\n",
    "        'dropout': [0.0, 0.2],\n",
    "        'weight_decay': [0, 1e-4],\n",
    "        'num_epochs': [20, 30, 50]\n",
    "    }\n",
    "-    Train_Shuffle = False\n",
    "-    Test_Shuffle = False\n",
    "\n",
    "- Best Hyperparameters: {'batch_size': 1, 'dropout': 0.2, 'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 50, 'num_layers': 4, 'weight_decay': 0}\n",
    "- Best Validation Accuracy: 85.00%\n",
    "- Best Average F1-Score for classes : ?? (Best Possible with 0.8004 along with 80.00% accuracy)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First LSTM Parameter Tuning on Non-Standardized Data:\n",
    "***\n",
    "Param:\n",
    "    param_grid = {\n",
    "        'hidden_size': [32, 64],\n",
    "        'num_layers': [2, 4],\n",
    "        'learning_rate': [0.001],\n",
    "        'batch_size': [1 ,2, 4],\n",
    "        'dropout': [0.0, 0.2],\n",
    "        'weight_decay': [0],\n",
    "        'num_epochs': [20, 30, 50]\n",
    "    }\n",
    "-    Train_Shuffle = False\n",
    "-    Test_Shuffle = False\n",
    "\n",
    "- Best Hyperparameters: {'batch_size': 4, 'dropout': 0.2, 'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 30, 'num_layers': 2, 'weight_decay': 0}\n",
    "- Best Validation Accuracy: 55.00%\n",
    "- Best Average F1-Score for classes : 0.6868\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Applying PCA function on training\n",
    "# # and testing set of X component\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=0.90)\n",
    "\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_pca = xgb.DMatrix(X_train_pca, label=y_train)\n",
    "# test_data_pca = xgb.DMatrix(X_test_pca, label=y_test)\n",
    "\n",
    "# xgb_model_pca = xgb.train(params, train_data_pca,num_boost_round=462)\n",
    "\n",
    "# # Predict the class labels\n",
    "# y_pred_pca = xgb_model_pca.predict(test_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate and print classification report\n",
    "# report = custom_classification_report(y_test, y_pred_pca, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# # creating a RF classifier\n",
    "# clf = RandomForestClassifier(n_estimators = 100)  \n",
    " \n",
    "# # Training the model on the training dataset\n",
    "# # fit function is used to train the model using the training sets as parameters\n",
    "# clf.fit(X_train_pca, y_train)\n",
    " \n",
    "# # performing predictions on the test dataset\n",
    "# y_pred_rf = clf.predict(X_test_pca)\n",
    " \n",
    "# custom_classification_report(y_test, y_pred_rf, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # creating a RF classifier\n",
    "# log_reg = LogisticRegression(random_state=42)\n",
    " \n",
    "# # Training the model on the training dataset\n",
    "# # fit function is used to train the model using the training sets as parameters\n",
    "# log_reg.fit(X_train_pca, y_train)\n",
    " \n",
    "# # performing predictions on the test dataset\n",
    "# y_pred_log_reg = log_reg.predict(X_test_pca)\n",
    " \n",
    "# custom_classification_report(y_test, y_pred_log_reg, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define the LSTM model\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        \n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])  # Extract the output from the last time step\n",
    "#         return out\n",
    "\n",
    "# # Prepare the data\n",
    "# X_train_pca_tensor = torch.tensor(X_train_pca, dtype=torch.float32).unsqueeze(1)\n",
    "# y_train_pca_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "# X_test_pca_tensor = torch.tensor(X_test_pca, dtype=torch.float32).unsqueeze(1)\n",
    "# y_test_pca_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# train_pca_dataset = TensorDataset(X_train_pca_tensor, y_train_pca_tensor)\n",
    "# test_pca_dataset = TensorDataset(X_test_pca_tensor, y_test_pca_tensor)\n",
    "\n",
    "# train_pca_loader = DataLoader(train_pca_dataset, batch_size=4, shuffle=True)\n",
    "# test_pca_loader = DataLoader(test_pca_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # Initialize the model, loss function, and optimizer\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_size = 128\n",
    "# num_layers = 4\n",
    "# num_classes = 3\n",
    "\n",
    "# model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 50\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         outputs = model(X_batch)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         y_true.extend(y_batch.numpy())\n",
    "#         y_pred.extend(predicted.numpy())\n",
    "\n",
    "# # Calculate F1 score and accuracy\n",
    "# f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "# print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
