{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transform_eeg_data = pd.read_csv('processed_data\\\\transformed_pca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be removed\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, target_names, class_indices):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Initialize lists to store precision, recall, F1-score, and support for each class\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "\n",
    "    # Calculate precision, recall, F1-score for each class\n",
    "    for i in range(len(target_names)):\n",
    "        true_positives = cm[i, i]\n",
    "        false_positives = cm[:, i].sum() - true_positives\n",
    "        false_negatives = cm[i, :].sum() - true_positives\n",
    "        true_negatives = cm.sum() - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        if true_positives + false_positives > 0:\n",
    "            precision_i = true_positives / (true_positives + false_positives)\n",
    "        else:\n",
    "            precision_i = 0.0\n",
    "\n",
    "        # Recall: TP / (TP + FN)\n",
    "        if true_positives + false_negatives > 0:\n",
    "            recall_i = true_positives / (true_positives + false_negatives)\n",
    "        else:\n",
    "            recall_i = 0.0\n",
    "\n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        if precision_i + recall_i > 0:\n",
    "            f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i)\n",
    "        else:\n",
    "            f1_i = 0.0\n",
    "\n",
    "        # Support: The number of true instances of each class\n",
    "        support_i = cm[i, :].sum()\n",
    "\n",
    "        # Append calculated metrics for this class\n",
    "        precision.append(precision_i)\n",
    "        recall.append(recall_i)\n",
    "        f1_score.append(f1_i)\n",
    "        support.append(support_i)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "    # Calculate average F1-score for specified classes\n",
    "    f1_average = np.mean([f1_score[i] for i in class_indices])\n",
    "\n",
    "    # Print the aesthetically improved report\n",
    "    print(\"\\n\" + \"Classification Report\".center(65, \"=\"))\n",
    "    print(f\"{'Class':<15}{'Precision':>12}{'Recall':>12}{'F1-Score':>12}{'Support':>12}\")\n",
    "    print(\"=\" * 65)\n",
    "    for i, label in enumerate(target_names):\n",
    "        print(f\"{label:<15}{precision[i]:>12.4f}{recall[i]:>12.4f}{f1_score[i]:>12.4f}{support[i]:>12}\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"Average F1-Score for classes : {f1_average:.4f}\")\n",
    "    print(f\"Accuracy : {accuracy:.4f}\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "class_indices = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "transform_eeg_data['GROUP'] = le.fit_transform(transform_eeg_data['GROUP'])\n",
    "# Convert object columns to numeric\n",
    "for col in transform_eeg_data.columns:\n",
    "    if transform_eeg_data[col].dtype == 'object':\n",
    "        transform_eeg_data[col] = pd.to_numeric(transform_eeg_data[col], errors='coerce')\n",
    "\n",
    "# Features and target\n",
    "X = transform_eeg_data.drop('GROUP', axis=1)\n",
    "y = transform_eeg_data['GROUP']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,  # Number of classes\n",
    "    'learning_rate': 0.001,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.6478457734751482,\n",
    "    'colsample_bytree': 0.9431214021788126,\n",
    "    # 'device': 'cuda',  # Use GPU\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "test_data = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "xgb_model = xgb.train(params, train_data,num_boost_round=462)\n",
    "\n",
    "# Predict the class labels\n",
    "y_pred = xgb_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected features: ['Unnamed: 0', 'f7', 'cp5', 'p3', 'o1', 'o2', 'fc6', 'cp3', 'af8', 'empty']\n",
      "Dataset columns: ['Unnamed: 0', 'f7', 'cp5', 'p3', 'o1', 'o2', 'fc6', 'cp3', 'af8', 'empty', 'GROUP']\n"
     ]
    }
   ],
   "source": [
    "print('Expected features:', xgb_model.feature_names)\n",
    "print('Dataset columns:', transform_eeg_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names=list(map(str,le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.7778      0.8750      0.8235           8\n",
      "1                    0.6364      0.7778      0.7000           9\n",
      "2                    0.0000      0.0000      0.0000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.5078\n",
      "Accuracy : 0.7000\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate and print classification report\n",
    "report = custom_classification_report(y_test, y_pred, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.7000      0.8750      0.7778           8\n",
      "1                    0.7000      0.7778      0.7368           9\n",
      "2                    0.0000      0.0000      0.0000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.5049\n",
      "Accuracy : 0.7000\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# creating a RF classifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)  \n",
    " \n",
    "# Training the model on the training dataset\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_rf = clf.predict(X_test)\n",
    " \n",
    "custom_classification_report(y_test, y_pred_rf, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================Classification Report======================\n",
      "Class             Precision      Recall    F1-Score     Support\n",
      "=================================================================\n",
      "0                    0.6667      0.7500      0.7059           8\n",
      "1                    0.6667      0.6667      0.6667           9\n",
      "2                    0.5000      0.3333      0.4000           3\n",
      "=================================================================\n",
      "Average F1-Score for classes : 0.5908\n",
      "Accuracy : 0.6500\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# creating a RF classifier\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    " \n",
    "# Training the model on the training dataset\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "log_reg.fit(X_train, y_train)\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    " \n",
    "custom_classification_report(y_test, y_pred_log_reg, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 10)"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.0844\n",
      "Epoch [2/50], Loss: 1.0598\n",
      "Epoch [3/50], Loss: 1.0254\n",
      "Epoch [4/50], Loss: 0.6307\n",
      "Epoch [5/50], Loss: 1.1819\n",
      "Epoch [6/50], Loss: 0.8507\n",
      "Epoch [7/50], Loss: 0.7664\n",
      "Epoch [8/50], Loss: 0.5546\n",
      "Epoch [9/50], Loss: 0.6613\n",
      "Epoch [10/50], Loss: 0.5728\n",
      "Epoch [11/50], Loss: 0.8226\n",
      "Epoch [12/50], Loss: 0.6433\n",
      "Epoch [13/50], Loss: 0.8756\n",
      "Epoch [14/50], Loss: 0.3103\n",
      "Epoch [15/50], Loss: 1.2916\n",
      "Epoch [16/50], Loss: 0.4477\n",
      "Epoch [17/50], Loss: 0.6149\n",
      "Epoch [18/50], Loss: 0.5584\n",
      "Epoch [19/50], Loss: 0.7529\n",
      "Epoch [20/50], Loss: 0.4820\n",
      "Epoch [21/50], Loss: 0.7213\n",
      "Epoch [22/50], Loss: 0.3418\n",
      "Epoch [23/50], Loss: 0.5584\n",
      "Epoch [24/50], Loss: 0.4458\n",
      "Epoch [25/50], Loss: 0.7534\n",
      "Epoch [26/50], Loss: 0.3004\n",
      "Epoch [27/50], Loss: 0.0457\n",
      "Epoch [28/50], Loss: 0.1519\n",
      "Epoch [29/50], Loss: 0.0441\n",
      "Epoch [30/50], Loss: 0.2120\n",
      "Epoch [31/50], Loss: 0.2350\n",
      "Epoch [32/50], Loss: 0.8322\n",
      "Epoch [33/50], Loss: 0.0823\n",
      "Epoch [34/50], Loss: 0.2149\n",
      "Epoch [35/50], Loss: 0.7742\n",
      "Epoch [36/50], Loss: 0.0980\n",
      "Epoch [37/50], Loss: 0.0699\n",
      "Epoch [38/50], Loss: 0.0595\n",
      "Epoch [39/50], Loss: 0.0588\n",
      "Epoch [40/50], Loss: 0.0160\n",
      "Epoch [41/50], Loss: 0.4628\n",
      "Epoch [42/50], Loss: 0.4591\n",
      "Epoch [43/50], Loss: 0.0846\n",
      "Epoch [44/50], Loss: 0.5299\n",
      "Epoch [45/50], Loss: 0.1784\n",
      "Epoch [46/50], Loss: 0.0064\n",
      "Epoch [47/50], Loss: 0.0386\n",
      "Epoch [48/50], Loss: 0.5246\n",
      "Epoch [49/50], Loss: 0.4191\n",
      "Epoch [50/50], Loss: 0.0286\n",
      "F1 Score: 0.5345\n",
      "Accuracy: 55.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Extract the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Prepare the data\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "num_classes = 3\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Calculate F1 score and accuracy\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Applying PCA function on training\n",
    "# # and testing set of X component\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=0.90)\n",
    "\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_pca = xgb.DMatrix(X_train_pca, label=y_train)\n",
    "# test_data_pca = xgb.DMatrix(X_test_pca, label=y_test)\n",
    "\n",
    "# xgb_model_pca = xgb.train(params, train_data_pca,num_boost_round=462)\n",
    "\n",
    "# # Predict the class labels\n",
    "# y_pred_pca = xgb_model_pca.predict(test_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate and print classification report\n",
    "# report = custom_classification_report(y_test, y_pred_pca, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# # creating a RF classifier\n",
    "# clf = RandomForestClassifier(n_estimators = 100)  \n",
    " \n",
    "# # Training the model on the training dataset\n",
    "# # fit function is used to train the model using the training sets as parameters\n",
    "# clf.fit(X_train_pca, y_train)\n",
    " \n",
    "# # performing predictions on the test dataset\n",
    "# y_pred_rf = clf.predict(X_test_pca)\n",
    " \n",
    "# custom_classification_report(y_test, y_pred_rf, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # creating a RF classifier\n",
    "# log_reg = LogisticRegression(random_state=42)\n",
    " \n",
    "# # Training the model on the training dataset\n",
    "# # fit function is used to train the model using the training sets as parameters\n",
    "# log_reg.fit(X_train_pca, y_train)\n",
    " \n",
    "# # performing predictions on the test dataset\n",
    "# y_pred_log_reg = log_reg.predict(X_test_pca)\n",
    " \n",
    "# custom_classification_report(y_test, y_pred_log_reg, target_names=target_names,class_indices = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define the LSTM model\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        \n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])  # Extract the output from the last time step\n",
    "#         return out\n",
    "\n",
    "# # Prepare the data\n",
    "# X_train_pca_tensor = torch.tensor(X_train_pca, dtype=torch.float32).unsqueeze(1)\n",
    "# y_train_pca_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "# X_test_pca_tensor = torch.tensor(X_test_pca, dtype=torch.float32).unsqueeze(1)\n",
    "# y_test_pca_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# train_pca_dataset = TensorDataset(X_train_pca_tensor, y_train_pca_tensor)\n",
    "# test_pca_dataset = TensorDataset(X_test_pca_tensor, y_test_pca_tensor)\n",
    "\n",
    "# train_pca_loader = DataLoader(train_pca_dataset, batch_size=4, shuffle=True)\n",
    "# test_pca_loader = DataLoader(test_pca_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # Initialize the model, loss function, and optimizer\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_size = 128\n",
    "# num_layers = 4\n",
    "# num_classes = 3\n",
    "\n",
    "# model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 50\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         outputs = model(X_batch)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         y_true.extend(y_batch.numpy())\n",
    "#         y_pred.extend(predicted.numpy())\n",
    "\n",
    "# # Calculate F1 score and accuracy\n",
    "# f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "# print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
